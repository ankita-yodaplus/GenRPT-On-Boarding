{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215179fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_23360\\611217850.py:19: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot-Product Attention is a type of attention mechanism used in the Transformer model. It involves computing the dot products of queries with keys, dividing them by the square root of the dimensionality, and applying a softmax function to obtain weights on the values. This mechanism is used to determine the compatibility between the query and key values, ultimately generating output values based on these computations. It is an efficient and space-saving method compared to other attention mechanisms, especially for larger values of dimensionality.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "# from langchain_milvus import Milvus\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# 1. Load and split documents\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 2. Generate embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# 3. Store in Milvus (connect to your Milvus instance)\n",
    "milvus_vectorstore = Milvus.from_documents(\n",
    "    documents[:30],\n",
    "    embedding,\n",
    "    connection_args={\n",
    "        \"host\": \"localhost\",  # or your IP or Zilliz endpoint\n",
    "        \"port\": \"19530\",\n",
    "    },\n",
    "    collection_name=\"attention_docs\"\n",
    ")\n",
    "\n",
    "# 4. Prepare the LLM and prompt\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "# 5. Create document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = milvus_vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# 6. Invoke with a query\n",
    "response = retrieval_chain.invoke({\"input\": \"Scaled Dot-Product Attention\"})\n",
    "print(response['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
